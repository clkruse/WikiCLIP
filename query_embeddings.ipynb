{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from typing import Dict, List, Optional, Union\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEmbeddingsReader(ABC):\n",
    "    \"\"\"Abstract base class for embedding readers.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 db_path: str = \"wiki_embeddings.db\",\n",
    "                 model_name: str = \"openai/clip-vit-base-patch32\"):\n",
    "        \"\"\"Initialize the base reader with common functionality.\"\"\"\n",
    "        self.db_path = db_path\n",
    "        self.device = self._get_device()\n",
    "        self.model = CLIPModel.from_pretrained(model_name).to(self.device)\n",
    "        self.processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_device() -> str:\n",
    "        \"\"\"Determine the appropriate device for computation.\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            return \"cuda\"\n",
    "        elif torch.backends.mps.is_available():\n",
    "            return \"mps\"\n",
    "        return \"cpu\"\n",
    "\n",
    "    def get_article_by_title(self, title: str) -> Optional[Dict]:\n",
    "        \"\"\"Get a specific article by its title.\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            cursor = conn.execute(\"\"\"\n",
    "                SELECT article_id, title, url, embedding, processed_date\n",
    "                FROM embeddings \n",
    "                WHERE title = ?\n",
    "            \"\"\", (title,))\n",
    "            \n",
    "            row = cursor.fetchone()\n",
    "            if row:\n",
    "                return {\n",
    "                    'article_id': row[0],\n",
    "                    'title': row[1],\n",
    "                    'url': row[2],\n",
    "                    'embedding': np.frombuffer(row[3], dtype=np.float32),\n",
    "                    'processed_date': row[4]\n",
    "                }\n",
    "            return None\n",
    "\n",
    "    def get_database_stats(self) -> Dict:\n",
    "        \"\"\"Get statistics about the database.\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            stats = {}\n",
    "            \n",
    "            cursor = conn.execute(\"SELECT COUNT(*) FROM embeddings\")\n",
    "            stats['total_articles'] = cursor.fetchone()[0]\n",
    "            \n",
    "            cursor = conn.execute(\"\"\"\n",
    "                SELECT title, processed_date \n",
    "                FROM embeddings \n",
    "                ORDER BY processed_date DESC \n",
    "                LIMIT 1\n",
    "            \"\"\")\n",
    "            row = cursor.fetchone()\n",
    "            if row:\n",
    "                stats['most_recent_article'] = {\n",
    "                    'title': row[0],\n",
    "                    'date': row[1]\n",
    "                }\n",
    "            \n",
    "            cursor = conn.execute(\"SELECT COUNT(*) FROM failed_articles\")\n",
    "            stats['failed_articles'] = cursor.fetchone()[0]\n",
    "            \n",
    "            return stats\n",
    "\n",
    "    def _find_similar_articles(self, query_embedding: np.ndarray, limit: int) -> List[Dict]:\n",
    "        \"\"\"Find similar articles based on embedding.\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            cursor = conn.execute(\"\"\"\n",
    "                SELECT article_id, title, url, embedding\n",
    "                FROM embeddings\n",
    "            \"\"\")\n",
    "            \n",
    "            results = [\n",
    "                {\n",
    "                    'article_id': row[0],\n",
    "                    'title': row[1],\n",
    "                    'url': row[2],\n",
    "                    'similarity': float(np.dot(query_embedding[0], np.frombuffer(row[3], dtype=np.float32)))\n",
    "                }\n",
    "                for row in cursor\n",
    "            ]\n",
    "        \n",
    "        results.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "        return results[:limit]\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_embedding(self, input_data: Union[str, str]) -> torch.Tensor:\n",
    "        \"\"\"Generate embedding for the input data.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_similar_articles(self, input_data: Union[str, str], limit: int = 5) -> List[Dict]:\n",
    "        \"\"\"Find similar articles based on input data.\"\"\"\n",
    "        pass\n",
    "\n",
    "class TextMatcher(BaseEmbeddingsReader):\n",
    "    \"\"\"Reader for text-based embeddings.\"\"\"\n",
    "    \n",
    "    def get_embedding(self, input_text: str) -> torch.Tensor:\n",
    "        \"\"\"Generate embedding for input text.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            inputs = self.processor(\n",
    "                text=[input_text],\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=77\n",
    "            )\n",
    "            text_features = self.model.get_text_features(\n",
    "                **{k: v.to(self.device) for k, v in inputs.items()}\n",
    "            )\n",
    "            return text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    def get_similar_articles(self, query: str, limit: int = 5) -> List[Dict]:\n",
    "        \"\"\"Find articles similar to the query text.\"\"\"\n",
    "        query_embedding = self.get_embedding(query).cpu().numpy()\n",
    "        return self._find_similar_articles(query_embedding, limit)\n",
    "\n",
    "class ImageMatcher(BaseEmbeddingsReader):\n",
    "    \"\"\"Reader for image-based embeddings.\"\"\"\n",
    "\n",
    "    def _load_image(self, image_path: str) -> Image.Image:\n",
    "        \"\"\"Load an image from either a local path or URL.\"\"\"\n",
    "        try:\n",
    "            if image_path.startswith(('http://', 'https://')):\n",
    "                response = requests.get(image_path, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                return Image.open(BytesIO(response.content))\n",
    "            return Image.open(image_path)\n",
    "        except (requests.exceptions.RequestException, Exception) as e:\n",
    "            raise Exception(f\"Error loading image: {str(e)}\")\n",
    "\n",
    "    def get_embedding(self, image_path: str) -> torch.Tensor:\n",
    "        \"\"\"Generate embedding for input image.\"\"\"\n",
    "        try:\n",
    "            image = self._load_image(image_path)\n",
    "            with torch.no_grad():\n",
    "                inputs = self.processor(\n",
    "                    images=image,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                image_features = self.model.get_image_features(\n",
    "                    **{k: v.to(self.device) for k, v in inputs.items()}\n",
    "                )\n",
    "                return image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error processing image: {str(e)}\")\n",
    "\n",
    "    def get_similar_articles(self, image_path: str, limit: int = 5) -> List[Dict]:\n",
    "        \"\"\"Find articles similar to the input image.\"\"\"\n",
    "        query_embedding = self.get_embedding(image_path).cpu().numpy()\n",
    "        return self._find_similar_articles(query_embedding, limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total_articles': 104686, 'most_recent_article': {'title': 'Ardeshir Tarapore', 'date': '2025-02-02 19:15:49'}, 'failed_articles': 0}\n"
     ]
    }
   ],
   "source": [
    "# print db stats\n",
    "text_reader = TextMatcher()\n",
    "image_reader = ImageMatcher()\n",
    "print(text_reader.get_database_stats())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aeroméxico Connect Flight 2431, https://en.wikipedia.org/wiki/Aerom%C3%A9xico_Connect_Flight_2431, 0.293\n",
      "Wainui Falls, https://en.wikipedia.org/wiki/Wainui_Falls, 0.284\n",
      "Chinhoyi Caves, https://en.wikipedia.org/wiki/Chinhoyi_Caves, 0.283\n",
      "2019 Alta helicopter crash, https://en.wikipedia.org/wiki/2019_Alta_helicopter_crash, 0.280\n",
      "Plitvice Lakes incident, https://en.wikipedia.org/wiki/Plitvice_Lakes_incident, 0.274\n",
      "Tham Luang cave rescue, https://en.wikipedia.org/wiki/Tham_Luang_cave_rescue, 0.274\n",
      "Cyclone Owen, https://en.wikipedia.org/wiki/Cyclone_Owen, 0.274\n",
      "Cyclone Nora, https://en.wikipedia.org/wiki/Cyclone_Nora, 0.272\n",
      "Cyclone Fakir, https://en.wikipedia.org/wiki/Cyclone_Fakir, 0.272\n",
      "Cyclone Wasa–Arthur, https://en.wikipedia.org/wiki/Cyclone_Wasa%E2%80%93Arthur, 0.270\n",
      "April 23, 1998, Albanian–Yugoslav border ambush, https://en.wikipedia.org/wiki/April_23%2C_1998%2C_Albanian%E2%80%93Yugoslav_border_ambush, 0.269\n",
      "2020 Zagreb flash flood, https://en.wikipedia.org/wiki/2020_Zagreb_flash_flood, 0.268\n",
      "Hurricane Pali, https://en.wikipedia.org/wiki/Hurricane_Pali, 0.266\n",
      "1992 European Community Monitor Mission helicopter downing, https://en.wikipedia.org/wiki/1992_European_Community_Monitor_Mission_helicopter_downing, 0.266\n",
      "2023 Wagner Group plane crash, https://en.wikipedia.org/wiki/2023_Wagner_Group_plane_crash, 0.266\n",
      "Effects of Hurricane Dorian in the Caribbean, https://en.wikipedia.org/wiki/Effects_of_Hurricane_Dorian_in_the_Caribbean, 0.264\n",
      "Cyclone Mekunu, https://en.wikipedia.org/wiki/Cyclone_Mekunu, 0.264\n",
      "2018 Hawaii floods, https://en.wikipedia.org/wiki/2018_Hawaii_floods, 0.264\n",
      "Cyclone Lili (2019), https://en.wikipedia.org/wiki/Cyclone_Lili_%282019%29, 0.263\n",
      "Vikos–Aoös National Park, https://en.wikipedia.org/wiki/Vikos%E2%80%93Ao%C3%B6s_National_Park, 0.263\n"
     ]
    }
   ],
   "source": [
    "# For text-based search\n",
    "#similar_articles = text_reader.get_similar_articles(\"ficus audrey\")\n",
    "#[print(article['title']) for article in similar_articles];\n",
    "\n",
    "# For image-based search\n",
    "similar_articles = image_reader.get_similar_articles(\"/Users/clkruse/Downloads/IMG_6984.jpeg\",20)\n",
    "[print(f\"{article['title']}, {article['url']}, {article['similarity']:.3f}\") for article in similar_articles];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Astronaut\n",
      "La Vie en rose\n",
      "Interstellar (film)\n",
      "First Man (film)\n",
      "Gravity (Lecrae album)\n",
      "Lucy in the Sky\n",
      "Space Oddity\n",
      "Lovely (Billie Eilish and Khalid song)\n",
      "Closer (The Chainsmokers song)\n",
      "Passenger\n",
      "Diana (Pop Smoke song)\n",
      "The Impossible Astronaut\n",
      "Mars in fiction\n",
      "Rocket (Beyoncé song)\n",
      "Space suit\n",
      "European Space Agency\n",
      "Curiosity (EP)\n",
      "2suit\n",
      "We choose to go to the Moon\n",
      "Wine and Roses\n"
     ]
    }
   ],
   "source": [
    "class BaseEmbeddingsReader(ABC):\n",
    "    \"\"\"Abstract base class for embedding readers.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 db_path: str = \"wiki_embeddings.db\",\n",
    "                 model_name: str = \"openai/clip-vit-base-patch32\"):\n",
    "        \"\"\"Initialize the base reader with common functionality.\"\"\"\n",
    "        self.db_path = db_path\n",
    "        self.device = self._get_device()\n",
    "        self.model = CLIPModel.from_pretrained(model_name).to(self.device)\n",
    "        self.processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_device() -> str:\n",
    "        \"\"\"Determine the appropriate device for computation.\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            return \"cuda\"\n",
    "        elif torch.backends.mps.is_available():\n",
    "            return \"mps\"\n",
    "        return \"cpu\"\n",
    "\n",
    "    def get_article_by_title(self, title: str) -> Optional[Dict]:\n",
    "        \"\"\"Get a specific article by its title.\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            cursor = conn.execute(\"\"\"\n",
    "                SELECT article_id, title, url, embedding, processed_date\n",
    "                FROM embeddings \n",
    "                WHERE title = ?\n",
    "            \"\"\", (title,))\n",
    "            \n",
    "            row = cursor.fetchone()\n",
    "            if row:\n",
    "                return {\n",
    "                    'article_id': row[0],\n",
    "                    'title': row[1],\n",
    "                    'url': row[2],\n",
    "                    'embedding': np.frombuffer(row[3], dtype=np.float32),\n",
    "                    'processed_date': row[4]\n",
    "                }\n",
    "            return None\n",
    "\n",
    "    def get_database_stats(self) -> Dict:\n",
    "        \"\"\"Get statistics about the database.\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            stats = {}\n",
    "            \n",
    "            cursor = conn.execute(\"SELECT COUNT(*) FROM embeddings\")\n",
    "            stats['total_articles'] = cursor.fetchone()[0]\n",
    "            \n",
    "            cursor = conn.execute(\"\"\"\n",
    "                SELECT title, processed_date \n",
    "                FROM embeddings \n",
    "                ORDER BY processed_date DESC \n",
    "                LIMIT 1\n",
    "            \"\"\")\n",
    "            row = cursor.fetchone()\n",
    "            if row:\n",
    "                stats['most_recent_article'] = {\n",
    "                    'title': row[0],\n",
    "                    'date': row[1]\n",
    "                }\n",
    "            \n",
    "            cursor = conn.execute(\"SELECT COUNT(*) FROM failed_articles\")\n",
    "            stats['failed_articles'] = cursor.fetchone()[0]\n",
    "            \n",
    "            return stats\n",
    "\n",
    "    def _find_similar_articles(self, query_embedding: np.ndarray, limit: int) -> List[Dict]:\n",
    "        \"\"\"Find similar articles based on embedding.\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            cursor = conn.execute(\"\"\"\n",
    "                SELECT article_id, title, url, embedding\n",
    "                FROM embeddings\n",
    "            \"\"\")\n",
    "            \n",
    "            results = [\n",
    "                {\n",
    "                    'article_id': row[0],\n",
    "                    'title': row[1],\n",
    "                    'url': row[2],\n",
    "                    'similarity': float(np.dot(query_embedding[0], np.frombuffer(row[3], dtype=np.float32)))\n",
    "                }\n",
    "                for row in cursor\n",
    "            ]\n",
    "        \n",
    "        results.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "        return results[:limit]\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_embedding(self, input_data: Union[str, str]) -> torch.Tensor:\n",
    "        \"\"\"Generate embedding for the input data.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_similar_articles(self, input_data: Union[str, str], limit: int = 5) -> List[Dict]:\n",
    "        \"\"\"Find similar articles based on input data.\"\"\"\n",
    "        pass\n",
    "\n",
    "class ImageMatcher(BaseEmbeddingsReader):\n",
    "    \"\"\"Reader for image-based embeddings.\"\"\"\n",
    "\n",
    "    def _load_image(self, image_path: str) -> Image.Image:\n",
    "        \"\"\"Load an image from either a local path or URL.\"\"\"\n",
    "        try:\n",
    "            if image_path.startswith(('http://', 'https://')):\n",
    "                response = requests.get(image_path, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                return Image.open(BytesIO(response.content))\n",
    "            return Image.open(image_path)\n",
    "        except (requests.exceptions.RequestException, Exception) as e:\n",
    "            raise Exception(f\"Error loading image: {str(e)}\")\n",
    "\n",
    "    def get_embedding(self, image_path: str) -> torch.Tensor:\n",
    "        \"\"\"Generate embedding for input image.\"\"\"\n",
    "        try:\n",
    "            image = self._load_image(image_path)\n",
    "            with torch.no_grad():\n",
    "                inputs = self.processor(\n",
    "                    images=image,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                image_features = self.model.get_image_features(\n",
    "                    **{k: v.to(self.device) for k, v in inputs.items()}\n",
    "                )\n",
    "                return image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error processing image: {str(e)}\")\n",
    "\n",
    "    def get_similar_articles(self, image_path: str, limit: int = 5) -> List[Dict]:\n",
    "        \"\"\"Find articles similar to the input image.\"\"\"\n",
    "        query_embedding = self.get_embedding(image_path).cpu().numpy()\n",
    "        return self._find_similar_articles(query_embedding, limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "core",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
