{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "import sqlite3\n",
    "import time\n",
    "from enum import Enum\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import logging\n",
    "import mwclient\n",
    "import hashlib\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database handling\n",
    "class EmbeddingsDatabase:\n",
    "    def __init__(self, db_path: str = \"wiki_embeddings.db\"):\n",
    "        self.db_path = db_path\n",
    "        self.setup_database()\n",
    "    \n",
    "    def setup_database(self):\n",
    "        \"\"\"Create database tables.\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            conn.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS embeddings (\n",
    "                    article_id TEXT PRIMARY KEY,\n",
    "                    title TEXT,\n",
    "                    url TEXT,\n",
    "                    embedding BLOB,\n",
    "                    processed_date TEXT,\n",
    "                    hash TEXT\n",
    "                )\n",
    "            \"\"\")\n",
    "            conn.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS failed_articles (\n",
    "                    article_id TEXT PRIMARY KEY,\n",
    "                    title TEXT,\n",
    "                    error_message TEXT,\n",
    "                    attempt_date TEXT\n",
    "                )\n",
    "            \"\"\")\n",
    "    \n",
    "    def store_embedding(self, article_id: str, title: str, url: str, embedding: bytes, hash_val: str):\n",
    "        \"\"\"Store a single embedding in the database.\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            conn.execute(\n",
    "                \"\"\"\n",
    "                INSERT OR REPLACE INTO embeddings \n",
    "                (article_id, title, url, embedding, processed_date, hash)\n",
    "                VALUES (?, ?, ?, ?, datetime('now'), ?)\n",
    "                \"\"\",\n",
    "                (article_id, title, url, embedding, hash_val)\n",
    "            )\n",
    "    \n",
    "    def store_failed_article(self, article_id: str, title: str, error: str):\n",
    "        \"\"\"Store information about failed article processing.\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            conn.execute(\n",
    "                \"\"\"\n",
    "                INSERT OR REPLACE INTO failed_articles \n",
    "                (article_id, title, error_message, attempt_date)\n",
    "                VALUES (?, ?, ?, datetime('now'))\n",
    "                \"\"\",\n",
    "                (article_id, title, error)\n",
    "            )\n",
    "\n",
    "# Wikipedia API handling\n",
    "class WikipediaArticleFetcher:\n",
    "    def __init__(self, user_agent: str):\n",
    "        self.site = mwclient.Site('en.wikipedia.org', clients_useragent=user_agent)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def get_vital_articles(self, level: int = 4) -> List[Dict]:\n",
    "        \"\"\"Get vital articles at specified level.\"\"\"\n",
    "        if not isinstance(level, int) or level < 1 or level > 5:\n",
    "            raise ValueError(\"Level must be an integer between 1 and 5\")\n",
    "        \n",
    "        article_titles = set()\n",
    "        \n",
    "        def process_article_lines(text: str, marker: str = '*') -> None:\n",
    "            for line in text.split('\\n'):\n",
    "                if line.startswith(marker) and '[[' in line and ']]' in line:\n",
    "                    start = line.find('[[') + 2\n",
    "                    end = line.find(']]')\n",
    "                    article_title = line[start:end]\n",
    "                    if '|' in article_title:\n",
    "                        article_title = article_title.split('|')[0]\n",
    "                    article_titles.add(article_title)\n",
    "        \n",
    "        if level in [4, 5]:\n",
    "            main_page = self.site.pages[f'Wikipedia:Vital articles/Level/{level}']\n",
    "            vital_article_categories = []\n",
    "            \n",
    "            for line in main_page.text().split('\\n'):\n",
    "                if line.startswith('|') and '[[' in line and ']]' in line:\n",
    "                    start = line.find('[[') + 2\n",
    "                    end = line.find(']]')\n",
    "                    category = line[start:end].split('|')[0]\n",
    "                    vital_article_categories.append(category)\n",
    "            \n",
    "            for category in vital_article_categories:\n",
    "                category_page = self.site.pages[category]\n",
    "                process_article_lines(category_page.text(), marker='#')\n",
    "        else:\n",
    "            page = self.site.pages[f'Wikipedia:Vital articles/Level/{level}']\n",
    "            process_article_lines(page.text())\n",
    "\n",
    "        return self._convert_titles_to_articles(sorted(article_titles))\n",
    "    \n",
    "    def get_good_articles(self) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get all articles that have been marked as good articles from\n",
    "        the page https://en.wikipedia.org/wiki/Wikipedia:Good_articles/all\n",
    "        \"\"\"\n",
    "\n",
    "        good_article_base = 'Wikipedia:Good_articles'\n",
    "        # start by getting all categories\n",
    "        categories = []\n",
    "        counter = 0\n",
    "        page = self.site.pages[good_article_base + '/all']\n",
    "        for line in page.text().split('\\n'):\n",
    "                #print(line)\n",
    "                if line.startswith('*') and '[[' in line and ']]' in line:\n",
    "                    start = line.find('[[') + 2\n",
    "                    end = line.find(']]')\n",
    "                    category = line[start:end].split('|')[1]\n",
    "                    categories.append(category)\n",
    "\n",
    "        # get all articles in the categories\n",
    "        articles = []\n",
    "        for category in categories:\n",
    "            page = self.site.pages[f'{good_article_base}/{category}']\n",
    "            for line in page.text().split('\\n'):\n",
    "                if '[[' in line and ']]' in line and '|' not in line:\n",
    "                    start = line.find('[[') + 2\n",
    "                    end = line.find(']]')\n",
    "                    article = line[start:end]\n",
    "                    articles.append(article)\n",
    "                    counter += 1\n",
    "                    if counter % 500 == 0:\n",
    "                        print(f\"Grabbed {counter} good article titles\")\n",
    "        \n",
    "        return self._convert_titles_to_articles(sorted(articles))\n",
    "    \n",
    "    def get_a_class_articles(self) -> List[Dict]:\n",
    "        \"\"\"Get all A-Class articles.\"\"\"\n",
    "        articles = []\n",
    "        counter = 0\n",
    "        for category in self.site.categories['A-Class_articles'].members():\n",
    "            if category.length > 0:\n",
    "                for member in category.members():\n",
    "                    title = member.name.split(':')[1]\n",
    "                    articles.append(title)\n",
    "                    counter += 1\n",
    "                    if counter % 500 == 0:\n",
    "                        print(f\"Grabbed {counter} A-Class article titles\")\n",
    "        return self._convert_titles_to_articles(sorted(articles))\n",
    "\n",
    "    def get_articles_from_titles(self, titles: List[str]) -> List[Dict]:\n",
    "        \"\"\"Get articles from a list of titles.\"\"\"\n",
    "        return self._convert_titles_to_articles(titles)\n",
    "    \n",
    "    def _convert_titles_to_articles(self, titles: List[str]) -> List[Dict]:\n",
    "        \"\"\"Convert titles to article dictionaries with metadata.\"\"\"\n",
    "        articles = []\n",
    "        for i in range(0, len(titles), 50):\n",
    "            batch_titles = titles[i:i + 50]\n",
    "            result = self.site.api('query', \n",
    "                                format='json',\n",
    "                                titles='|'.join(batch_titles),\n",
    "                                redirects=1)\n",
    "            \n",
    "            if 'query' in result and 'pages' in result['query']:\n",
    "                for page_id, page_data in result['query']['pages'].items():\n",
    "                    if 'missing' not in page_data and 'invalid' not in page_data:\n",
    "                        title = page_data['title']\n",
    "                        title_encoded = urllib.parse.quote(title.replace(' ', '_'))\n",
    "                        articles.append({\n",
    "                            'id': str(page_id),\n",
    "                            'title': title,\n",
    "                            'url': f\"https://en.wikipedia.org/wiki/{title_encoded}\"\n",
    "                        })\n",
    "            time.sleep(0.01)\n",
    "        return articles\n",
    "    \n",
    "    def get_article_content(self, articles: List[Dict]) -> List[Tuple[Dict, str, str]]:\n",
    "        \"\"\"Fetch content for a batch of articles.\"\"\"\n",
    "        results = []\n",
    "        for i in range(0, len(articles), 50):\n",
    "            chunk = articles[i:i + 50]\n",
    "            titles = '|'.join(article['title'] for article in chunk)\n",
    "            \n",
    "            params = {\n",
    "                'format': 'json',\n",
    "                'prop': 'extracts',\n",
    "                'exintro': True,\n",
    "                'explaintext': True,\n",
    "                'redirects': 1,\n",
    "                'titles': titles\n",
    "            }\n",
    "            \n",
    "            result = self.site.api('query', **params)\n",
    "            \n",
    "            if 'query' in result and 'pages' in result['query']:\n",
    "                for page_id, page_data in result['query']['pages'].items():\n",
    "                    matching_article = next(\n",
    "                        (a for a in chunk if str(page_data.get('pageid')) == a['id']),\n",
    "                        None\n",
    "                    )\n",
    "                    \n",
    "                    if matching_article:\n",
    "                        content = page_data.get('extract', '').strip()\n",
    "                        if not content:\n",
    "                            content = page_data.get('title', matching_article['title'])\n",
    "                        \n",
    "                        if content:\n",
    "                            hash_val = hashlib.sha256(content.encode()).hexdigest()\n",
    "                            results.append((matching_article, content, hash_val))\n",
    "            \n",
    "            time.sleep(0.01)\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Embedding generation\n",
    "class EmbeddingGenerator:\n",
    "    def __init__(self, model_name: str = \"openai/clip-vit-base-patch32\"):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "        self.model = CLIPModel.from_pretrained(model_name).to(self.device)\n",
    "        self.processor = CLIPProcessor.from_pretrained(model_name)\n",
    "    \n",
    "    def generate_embeddings(self, texts: List[str]) -> torch.Tensor:\n",
    "        \"\"\"Generate embeddings for a list of texts.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            inputs = self.processor(\n",
    "                text=texts,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=77\n",
    "            )\n",
    "            text_features = self.model.get_text_features(\n",
    "                **{k: v.to(self.device) for k, v in inputs.items()}\n",
    "            )\n",
    "            return text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "class ArticleSource(Enum):\n",
    "    VITAL = \"vital\"\n",
    "    GOOD = \"good\" \n",
    "    CUSTOM = \"custom\"\n",
    "    A_CLASS = \"a_class\"\n",
    "\n",
    "# Main orchestrator\n",
    "class WikiEmbeddingsOrchestrator:\n",
    "    def __init__(self, \n",
    "                 model_name: str = \"openai/clip-vit-base-patch32\",\n",
    "                 db_path: str = \"wiki_embeddings.db\",\n",
    "                 batch_size: int = 32,\n",
    "                 user_agent: str = 'clip_embedding_wikipedia/0.0 (https://calebkruse.com/; caleb.krs@gmail.com)'\n",
    "    ):\n",
    "        # Setup logging\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler('wiki_embeddings.log'),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Initialize components\n",
    "        self.batch_size = batch_size\n",
    "        self.db = EmbeddingsDatabase(db_path)\n",
    "        self.wiki_fetcher = WikipediaArticleFetcher(user_agent)\n",
    "        self.embedding_generator = EmbeddingGenerator(model_name)\n",
    "\n",
    "    def _get_articles_from_source(self, \n",
    "                                source: ArticleSource,\n",
    "                                vital_level: Optional[int] = None,\n",
    "                                custom_titles: Optional[List[str]] = None) -> List[Dict]:\n",
    "        \"\"\"Get articles based on the specified source type.\"\"\"\n",
    "        if source == ArticleSource.VITAL:\n",
    "            if not vital_level or not (1 <= vital_level <= 5):\n",
    "                raise ValueError(\"Vital articles require a level between 1 and 5\")\n",
    "            return self.wiki_fetcher.get_vital_articles(level=vital_level)\n",
    "        \n",
    "        elif source == ArticleSource.GOOD:\n",
    "            return self.wiki_fetcher.get_good_articles()\n",
    "        \n",
    "        elif source == ArticleSource.CUSTOM:\n",
    "            if not custom_titles:\n",
    "                raise ValueError(\"Custom article source requires a list of titles\")\n",
    "            return self.wiki_fetcher.get_articles_from_titles(custom_titles)\n",
    "        \n",
    "        elif source == ArticleSource.A_CLASS:\n",
    "            return self.wiki_fetcher.get_a_class_articles()\n",
    "        \n",
    "        raise ValueError(f\"Unknown article source: {source}\")\n",
    "\n",
    "    def process_batch(self, articles: List[Dict]):\n",
    "        \"\"\"Process a batch of articles.\"\"\"\n",
    "        try:\n",
    "            # Fetch article content\n",
    "            article_data = self.wiki_fetcher.get_article_content(articles)\n",
    "            \n",
    "            if not article_data:\n",
    "                self.logger.warning(\"No valid content found in batch\")\n",
    "                return\n",
    "            \n",
    "            # Split data\n",
    "            valid_articles, contents, hashes = zip(*article_data)\n",
    "            \n",
    "            # Generate embeddings\n",
    "            embeddings = self.embedding_generator.generate_embeddings(contents)\n",
    "            \n",
    "            # Store results\n",
    "            for article, embedding, hash_val in zip(valid_articles, embeddings, hashes):\n",
    "                self.db.store_embedding(\n",
    "                    article['id'],\n",
    "                    article['title'],\n",
    "                    article['url'],\n",
    "                    embedding.cpu().numpy().tobytes(),\n",
    "                    hash_val\n",
    "                )\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Batch processing error: {str(e)}\")\n",
    "            for article in articles:\n",
    "                self.db.store_failed_article(article['id'], article['title'], str(e))\n",
    "\n",
    "    def generate_embeddings(self,\n",
    "                          source: ArticleSource,\n",
    "                          vital_level: Optional[int] = None,\n",
    "                          custom_titles: Optional[List[str]] = None):\n",
    "        \"\"\"Generate embeddings for articles from the specified source.\"\"\"\n",
    "        try:\n",
    "            articles_to_process = self._get_articles_from_source(\n",
    "                source=source,\n",
    "                vital_level=vital_level,\n",
    "                custom_titles=custom_titles\n",
    "            )\n",
    "            \n",
    "            if not articles_to_process:\n",
    "                self.logger.error(\"No articles found\")\n",
    "                return\n",
    "            \n",
    "            # Process in batches\n",
    "            total_articles = len(articles_to_process)\n",
    "            self.logger.info(f\"Found {total_articles} articles to process\")\n",
    "            \n",
    "            for i in range(0, total_articles, self.batch_size):\n",
    "                batch = articles_to_process[i:i + self.batch_size]\n",
    "                self.process_batch(batch)\n",
    "                self.logger.info(f\"Processed {min(i + self.batch_size, total_articles)}/{total_articles} articles\")\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            self.logger.info(\"Interrupted by user. Saving progress...\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Fatal error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_a_class_articles():\n",
    "    site = mwclient.Site('en.wikipedia.org', clients_useragent='clip_embedding_wikipedia/0.0 (https://calebkruse.com/; caleb.krs@gmail.com)')\n",
    "    articles = []\n",
    "    counter = 0\n",
    "    for category in site.categories['A-Class_articles'].members():\n",
    "        #print(category.name)\n",
    "        if category.length > 0:\n",
    "            for member in category.members():\n",
    "                title = member.name.split(':')[1]\n",
    "                articles.append(title)\n",
    "                counter += 1\n",
    "                if counter % 500 == 0:\n",
    "                    print(f\"Grabbed {counter} A-Class article titles\")\n",
    "    return articles\n",
    "\n",
    "def get_good_class_articles():\n",
    "    site = mwclient.Site('en.wikipedia.org', clients_useragent='clip_embedding_wikipedia/0.0 (https://calebkruse.com/; caleb.krs@gmail.com)')\n",
    "    articles = []\n",
    "    counter = 0\n",
    "    for category in site.categories['GA-Class_articles'].members():\n",
    "        if category.length > 0:\n",
    "            for member in category.members():\n",
    "                try:\n",
    "                    title = member.name.split(':')[1]\n",
    "                    articles.append(title)\n",
    "                    counter += 1\n",
    "                    if counter % 500 == 0:\n",
    "                        print(f\"Grabbed {counter} good article titles\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {member.name}: {e}\")\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_articles = get_good_class_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orchestrator = WikiEmbeddingsOrchestrator()\n",
    "\n",
    "# For vital articles\n",
    "#orchestrator.generate_embeddings(source=ArticleSource.VITAL, vital_level=2)\n",
    "\n",
    "# For good articles\n",
    "#orchestrator.generate_embeddings(source=ArticleSource.GOOD)\n",
    "\n",
    "# For A-Class articles\n",
    "#orchestrator.generate_embeddings(source=ArticleSource.A_CLASS)\n",
    "\n",
    "# For custom article list\n",
    "\n",
    "#titles = [\"Python (programming language)\", \"Machine learning\", \"Artificial intelligence\"]\n",
    "orchestrator.generate_embeddings(source=ArticleSource.CUSTOM, custom_titles=good_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic initialization\n",
    "orchestrator = WikiEmbeddingsOrchestrator(\n",
    "    model_name=\"openai/clip-vit-base-patch32\",\n",
    "    db_path=\"wiki_embeddings.db\",\n",
    "    batch_size=128\n",
    ")\n",
    "\n",
    "# Generate embeddings for vital articles (level 4)\n",
    "orchestrator.generate_embeddings(vital_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = set()\n",
    "\n",
    "for year in range(2016, 2024):\n",
    "    print(year)\n",
    "    # get all the months with 31 days\n",
    "    for month in [1,3,5,7,8,10,12]:\n",
    "        top_articles = get_top_articles(1000, month, year)\n",
    "        articles.update(top_articles['article'].tolist())\n",
    "articles = list(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = WikiEmbeddingsGenerator()\n",
    "generator.generate_embeddings(articles=articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and search\n",
    "reader = WikiEmbeddingsReader()\n",
    "similar = reader.get_similar_articles(\"computer mouse\")\n",
    "for article in similar:\n",
    "    print(f\"{article['title']}: {article['similarity']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the reader\n",
    "reader = ImageEmbeddingsReader()\n",
    "image_url = \"/Users/clkruse/Downloads/ginger_test.png\"\n",
    "# Find similar articles for an image\n",
    "similar_articles = reader.get_similar_articles_by_image(image_url, limit=5)\n",
    "similar_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mwviews.api\n",
    "import pandas as pd\n",
    "\n",
    "def get_top_articles(limit=100, month=12, year=2024):\n",
    "    # Initialize the PageviewsClient\n",
    "    user_agent = 'clip_embedding_wikipedia/0.0 (https://calebkruse.com/; caleb.krs@gmail.com)'\n",
    "    client = mwviews.api.PageviewsClient(user_agent=user_agent)\n",
    "    \n",
    "    # Get daily views for all articles\n",
    "    views = client.top_articles('en.wikipedia', limit=limit, month=month,year=year)\n",
    "    \n",
    "    return pd.DataFrame(views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = set()\n",
    "\n",
    "for year in range(2016, 2024):\n",
    "    print(year)\n",
    "    # get all the months with 31 days\n",
    "    for month in [1,3,5,7,8,10,12]:\n",
    "        top_articles = get_top_articles(1000, month, year)\n",
    "        articles.update(top_articles['article'].tolist())\n",
    "articles = list(articles)\n",
    "articles[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get embeddings for all the articles\n",
    "embedder = WikiEmbeddingsGenerator()\n",
    "embedder.generate_embeddings(articles=articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = mwclient.Site('en.wikipedia.org', clients_useragent='clip_embedding_wikipedia/0.0 (https://calebkruse.com/; caleb.krs@gmail.com)')\n",
    "\n",
    "good_article_base = 'Category:Articles by quality'\n",
    "# start by getting all categories\n",
    "categories = []\n",
    "page = site.pages[good_article_base]\n",
    "for line in page.text().split('\\n'):\n",
    "        print(line)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = []\n",
    "counter = 0\n",
    "for category in site.categories['A-Class_articles'].members():\n",
    "    #print(category.name)\n",
    "    if category.length > 0:\n",
    "        for member in category.members():\n",
    "            title = member.name.split(':')[1]\n",
    "            articles.append(title)\n",
    "            counter += 1\n",
    "            if counter % 500 == 0:\n",
    "                print(f\"Grabbed {counter} A-Class article titles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = mwclient.Site('en.wikipedia.org', clients_useragent='clip_embedding_wikipedia/0.0 (https://calebkruse.com/; caleb.krs@gmail.com)')\n",
    "\n",
    "good_article_base = 'Category:A-Class_articles'\n",
    "# start by getting all categories\n",
    "categories = []\n",
    "page = site.pages[good_article_base + '/all']\n",
    "for line in page.text().split('\\n'):\n",
    "        #print(line)\n",
    "        if line.startswith('*') and '[[' in line and ']]' in line:\n",
    "            start = line.find('[[') + 2\n",
    "            end = line.find(']]')\n",
    "            category = line[start:end].split('|')[1]\n",
    "            categories.append(category)\n",
    "\n",
    "# get all articles in the categories\n",
    "articles = []\n",
    "for category in categories:\n",
    "    page = site.pages[f'{good_article_base}/{category}']\n",
    "    for line in page.text().split('\\n'):\n",
    "        if '[[' in line and ']]' in line and '|' not in line:\n",
    "            start = line.find('[[') + 2\n",
    "            end = line.find(']]')\n",
    "            article = line[start:end]\n",
    "            articles.append(article)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "articles = []\n",
    "page = site.pages['Wikipedia:Good articles/Agriculture, food and drink']\n",
    "for line in page.text().split('\\n'):\n",
    "    #print(line)\n",
    "    if '[[' in line and ']]' in line and '|' not in line:\n",
    "        start = line.find('[[') + 2\n",
    "        end = line.find(']]')\n",
    "        article = line[start:end]\n",
    "        articles.append(article)\n",
    "        \n",
    "print(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mwclient\n",
    "import re\n",
    "from time import sleep\n",
    "\n",
    "def get_good_articles():\n",
    "    # Connect to English Wikipedia\n",
    "    site = mwclient.Site('en.wikipedia.org')\n",
    "    \n",
    "    # Dictionary to store articles by category\n",
    "    articles_by_category = {}\n",
    "    \n",
    "    # List of all category pages that contain good articles\n",
    "    category_pages = [\n",
    "        'Wikipedia:Good articles/Agriculture, food and drink',\n",
    "        'Wikipedia:Good articles/Art and architecture',\n",
    "        'Wikipedia:Good articles/Engineering and technology',\n",
    "        'Wikipedia:Good articles/Geography and places',\n",
    "        'Wikipedia:Good articles/History',\n",
    "        'Wikipedia:Good articles/Language and literature',\n",
    "        'Wikipedia:Good articles/Mathematics, science, and technology',\n",
    "        'Wikipedia:Good articles/Media and drama',\n",
    "        'Wikipedia:Good articles/Music',\n",
    "        'Wikipedia:Good articles/Natural sciences',\n",
    "        'Wikipedia:Good articles/Philosophy and religion',\n",
    "        'Wikipedia:Good articles/Social sciences and society',\n",
    "        'Wikipedia:Good articles/Sports and recreation',\n",
    "        'Wikipedia:Good articles/Video games',\n",
    "        'Wikipedia:Good articles/Warfare'\n",
    "    ]\n",
    "    \n",
    "    def extract_articles_from_text(text):\n",
    "        # Regular expression to find article links, excluding categories and templates\n",
    "        # Look for lines that start with * or # followed by a link\n",
    "        article_pattern = r'[\\*#]\\s*\\[\\[([^]|]+?)(?:\\|[^]]+)?\\]\\]'\n",
    "        matches = re.finditer(article_pattern, text)\n",
    "        \n",
    "        articles = []\n",
    "        for match in matches:\n",
    "            title = match.group(1)\n",
    "            # Skip Wikipedia: namespace and other special pages\n",
    "            if not any(title.startswith(prefix) for prefix in ['Wikipedia:', 'Template:', 'Category:', 'Portal:']):\n",
    "                articles.append(title)\n",
    "        return articles\n",
    "    \n",
    "    # Process each category page\n",
    "    for category_page_title in category_pages:\n",
    "        try:\n",
    "            print(f\"Processing {category_page_title}...\")\n",
    "            page = site.pages[category_page_title]\n",
    "            text = page.text()\n",
    "            print(text)\n",
    "            \n",
    "            # Extract articles from this category\n",
    "            category_articles = extract_articles_from_text(text)\n",
    "            \n",
    "            # Store articles under the category name (removing the prefix)\n",
    "            category_name = category_page_title.replace('Wikipedia:Good articles/', '')\n",
    "            articles_by_category[category_name] = category_articles\n",
    "            \n",
    "            # Be nice to Wikipedia's servers\n",
    "            sleep(1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {category_page_title}: {e}\")\n",
    "    \n",
    "    return articles_by_category\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        articles_by_category = get_good_articles()\n",
    "        print(articles_by_category)\n",
    "        \n",
    "        # Print summary and save results\n",
    "        total_articles = sum(len(articles) for articles in articles_by_category.values())\n",
    "        print(f\"\\nFound {total_articles} good articles across {len(articles_by_category)} categories:\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access Wikipedia fetcher directly\n",
    "wiki_fetcher = orchestrator.wiki_fetcher\n",
    "vital_articles = wiki_fetcher.get_vital_articles(level=3)\n",
    "\n",
    "vital_articles[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "core",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
