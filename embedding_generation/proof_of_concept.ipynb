{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import wikipedia\n",
    "import torch\n",
    "from typing import List, Tuple, Union, Optional\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiImageMatcher:\n",
    "    def __init__(self, model_name: str = \"openai/clip-vit-base-patch32\"):\n",
    "        \"\"\"\n",
    "        Initialize the WikiImageMatcher with a CLIP model from Hugging Face.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Name of the CLIP model from Hugging Face\n",
    "        \"\"\"\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "        self.model = CLIPModel.from_pretrained(model_name).to(self.device)\n",
    "        self.processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "    def clean_wiki_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean Wikipedia text by removing references, links, and extra whitespace.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Raw Wikipedia text\n",
    "            \n",
    "        Returns:\n",
    "            str: Cleaned text\n",
    "        \"\"\"\n",
    "        # Remove references\n",
    "        text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "        # Remove multiple newlines\n",
    "        text = re.sub(r'\\n+', ' ', text)\n",
    "        # Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        return text\n",
    "    \n",
    "    def find_wiki_article(self, query: str) -> Optional[wikipedia.WikipediaPage]:\n",
    "        \"\"\"\n",
    "        Find a Wikipedia article using multiple search strategies.\n",
    "        \n",
    "        Args:\n",
    "            query (str): Article title or search query\n",
    "            \n",
    "        Returns:\n",
    "            Optional[wikipedia.WikipediaPage]: Wikipedia page if found, None otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Strategy 1: Try direct page lookup\n",
    "            try:\n",
    "                return wikipedia.page(query, auto_suggest=False)\n",
    "            except (wikipedia.exceptions.PageError, wikipedia.exceptions.DisambiguationError):\n",
    "                pass\n",
    "\n",
    "            # Strategy 2: Try with auto_suggest\n",
    "            try:\n",
    "                return wikipedia.page(query, auto_suggest=True)\n",
    "            except wikipedia.exceptions.DisambiguationError as e:\n",
    "                # If disambiguation page, try the first suggestion\n",
    "                try:\n",
    "                    return wikipedia.page(e.options[0], auto_suggest=False)\n",
    "                except:\n",
    "                    pass\n",
    "            except wikipedia.exceptions.PageError:\n",
    "                pass\n",
    "\n",
    "            # Strategy 3: Search and use the first result\n",
    "            search_results = wikipedia.search(query, results=5)\n",
    "            if search_results:\n",
    "                try:\n",
    "                    return wikipedia.page(search_results[0], auto_suggest=False)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            # If all strategies fail\n",
    "            return None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error searching for '{query}': {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def get_article_embedding(self, article_name: str) -> Tuple[Optional[torch.Tensor], Optional[str]]:\n",
    "        \"\"\"\n",
    "        Get CLIP embedding for a Wikipedia article.\n",
    "        \n",
    "        Args:\n",
    "            article_name (str): Name of the Wikipedia article\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[Optional[torch.Tensor], Optional[str]]: \n",
    "                (embedding vector, actual article title) if successful,\n",
    "                (None, None) if failed\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Find the article\n",
    "            page = self.find_wiki_article(article_name)\n",
    "            if page is None:\n",
    "                print(f\"Could not find article: {article_name}\")\n",
    "                return None, None\n",
    "\n",
    "            # Get and clean content\n",
    "            content = self.clean_wiki_text(page.content)\n",
    "            \n",
    "            # Process text through CLIP\n",
    "            with torch.no_grad():\n",
    "                inputs = self.processor(\n",
    "                    text=[content],\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=77\n",
    "                )\n",
    "                text_features = self.model.get_text_features(**{k: v.to(self.device) for k, v in inputs.items()})\n",
    "                normalized_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "                \n",
    "            return normalized_features, page.title\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing article '{article_name}': {str(e)}\")\n",
    "            return None, None\n",
    "\n",
    "    \n",
    "    def get_image_embedding(self, image: Union[str, Image.Image]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Get CLIP embedding for an image.\n",
    "        \n",
    "        Args:\n",
    "            image (Union[str, Image.Image]): Either a path to an image or a PIL Image\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Embedding vector for the image\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load image if path is provided\n",
    "            if isinstance(image, str):\n",
    "                image = Image.open(image)\n",
    "                \n",
    "            # Process image through CLIP\n",
    "            with torch.no_grad():\n",
    "                inputs = self.processor(\n",
    "                    images=image,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                image_features = self.model.get_image_features(**{k: v.to(self.device) for k, v in inputs.items()})\n",
    "                \n",
    "            return image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error processing image: {str(e)}\")\n",
    "    \n",
    "    def find_matches(self, \n",
    "                    query_embedding: torch.Tensor,\n",
    "                    article_embeddings: List[Tuple[str, torch.Tensor]],\n",
    "                    num_matches: int = 1) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Find the closest matching articles for a query embedding.\n",
    "        \n",
    "        Args:\n",
    "            query_embedding (torch.Tensor): Embedding vector to match against\n",
    "            article_embeddings (List[Tuple[str, torch.Tensor]]): List of (article_name, embedding) pairs\n",
    "            num_matches (int): Number of matches to return\n",
    "            \n",
    "        Returns:\n",
    "            List[Tuple[str, float]]: List of (article_name, similarity_score) pairs\n",
    "        \"\"\"\n",
    "        similarities = []\n",
    "        \n",
    "        for article_name, article_embedding in article_embeddings:\n",
    "            similarity = torch.nn.functional.cosine_similarity(\n",
    "                query_embedding, article_embedding\n",
    "            ).item()\n",
    "            similarities.append((article_name, similarity))\n",
    "        \n",
    "        # Sort by similarity score in descending order\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return similarities[:num_matches]\n",
    "\n",
    "# Example usage function\n",
    "def demo_wiki_image_matcher(image_path: str, article_names: List[str], num_matches: int = 1):\n",
    "    \"\"\"\n",
    "    Demonstrate the WikiImageMatcher with example usage.\n",
    "    \"\"\"\n",
    "    matcher = WikiImageMatcher()\n",
    "    \n",
    "    # Get embeddings for all articles\n",
    "    article_embeddings = []\n",
    "    for article_name in article_names:\n",
    "        embedding, actual_title = matcher.get_article_embedding(article_name)\n",
    "        if embedding is not None and actual_title is not None:\n",
    "            article_embeddings.append((actual_title, embedding))\n",
    "        else:\n",
    "            print(f\"Skipping '{article_name}' due to retrieval error\")\n",
    "    \n",
    "    if not article_embeddings:\n",
    "        print(\"No valid articles found to match against\")\n",
    "        return []\n",
    "\n",
    "    # Get embedding for the query image\n",
    "    image_embedding = matcher.get_image_embedding(image_path)\n",
    "    \n",
    "    # Find matches\n",
    "    matches = matcher.find_matches(image_embedding, article_embeddings, num_matches)\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Astronaut', 0.27501875162124634),\n",
       " ('Shades of pink', 0.2281285524368286),\n",
       " ('Rose', 0.22468915581703186),\n",
       " ('Ancient Greek', 0.2177101969718933),\n",
       " ('Mount Everest', 0.17938131093978882),\n",
       " ('Midjourney', 0.1769527792930603)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = '/Users/clkruse/Downloads/astro_test.png'\n",
    "demo_wiki_image_matcher(image_path, [\"Astronaut\", \"Ancient Greek\", \"Roses\", \"Mount Everest\", \"Midjourney\", \"Pink (Color)\"], num_matches=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "DatasetNotFoundError",
     "evalue": "Dataset 'facebook/contrastive_search_index' doesn't exist on the Hub or cannot be accessed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDatasetNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfacebook/contrastive_search_index\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstreaming\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/core/lib/python3.10/site-packages/datasets/load.py:2129\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2124\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   2125\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2126\u001b[0m )\n\u001b[1;32m   2128\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2129\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2143\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2144\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2146\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/miniforge3/envs/core/lib/python3.10/site-packages/datasets/load.py:1849\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   1847\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1848\u001b[0m     download_config\u001b[38;5;241m.\u001b[39mstorage_options\u001b[38;5;241m.\u001b[39mupdate(storage_options)\n\u001b[0;32m-> 1849\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1850\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1851\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1852\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1853\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1858\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_custom_configs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1862\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[0;32m~/miniforge3/envs/core/lib/python3.10/site-packages/datasets/load.py:1719\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt reach the Hugging Face Hub for dataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, (DataFilesNotFoundError, DatasetNotFoundError, EmptyDatasetError)):\n\u001b[0;32m-> 1719\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1720\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[1;32m   1721\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trust_remote_code:\n",
      "File \u001b[0;32m~/miniforge3/envs/core/lib/python3.10/site-packages/datasets/load.py:1645\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1641\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetNotFoundError(\n\u001b[1;32m   1642\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRevision \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist for dataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1643\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1644\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1645\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetNotFoundError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist on the Hub or cannot be accessed.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1646\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1647\u001b[0m     dataset_script_path \u001b[38;5;241m=\u001b[39m api\u001b[38;5;241m.\u001b[39mhf_hub_download(\n\u001b[1;32m   1648\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mpath,\n\u001b[1;32m   1649\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1652\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mproxies,\n\u001b[1;32m   1653\u001b[0m     )\n",
      "\u001b[0;31mDatasetNotFoundError\u001b[0m: Dataset 'facebook/contrastive_search_index' doesn't exist on the Hub or cannot be accessed."
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"facebook/contrastive_search_index\", streaming=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "core",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
