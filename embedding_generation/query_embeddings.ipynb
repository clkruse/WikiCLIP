{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from typing import Dict, List, Optional, Union\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEmbeddingsReader(ABC):\n",
    "    \"\"\"Abstract base class for embedding readers.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 db_path: str = \"wiki_embeddings.db\",\n",
    "                 model_name: str = \"openai/clip-vit-base-patch32\"):\n",
    "        \"\"\"Initialize the base reader with common functionality.\"\"\"\n",
    "        self.db_path = db_path\n",
    "        self.device = self._get_device()\n",
    "        self.model = CLIPModel.from_pretrained(model_name).to(self.device)\n",
    "        self.processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_device() -> str:\n",
    "        \"\"\"Determine the appropriate device for computation.\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            return \"cuda\"\n",
    "        elif torch.backends.mps.is_available():\n",
    "            return \"mps\"\n",
    "        return \"cpu\"\n",
    "\n",
    "    def get_article_by_title(self, title: str) -> Optional[Dict]:\n",
    "        \"\"\"Get a specific article by its title.\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            cursor = conn.execute(\"\"\"\n",
    "                SELECT article_id, title, url, embedding, processed_date\n",
    "                FROM embeddings \n",
    "                WHERE title = ?\n",
    "            \"\"\", (title,))\n",
    "            \n",
    "            row = cursor.fetchone()\n",
    "            if row:\n",
    "                return {\n",
    "                    'article_id': row[0],\n",
    "                    'title': row[1],\n",
    "                    'url': row[2],\n",
    "                    'embedding': np.frombuffer(row[3], dtype=np.float32),\n",
    "                    'processed_date': row[4]\n",
    "                }\n",
    "            return None\n",
    "\n",
    "    def get_database_stats(self) -> Dict:\n",
    "        \"\"\"Get statistics about the database.\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            stats = {}\n",
    "            \n",
    "            cursor = conn.execute(\"SELECT COUNT(*) FROM embeddings\")\n",
    "            stats['total_articles'] = cursor.fetchone()[0]\n",
    "            \n",
    "            cursor = conn.execute(\"\"\"\n",
    "                SELECT title, processed_date \n",
    "                FROM embeddings \n",
    "                ORDER BY processed_date DESC \n",
    "                LIMIT 1\n",
    "            \"\"\")\n",
    "            row = cursor.fetchone()\n",
    "            if row:\n",
    "                stats['most_recent_article'] = {\n",
    "                    'title': row[0],\n",
    "                    'date': row[1]\n",
    "                }\n",
    "            \n",
    "            cursor = conn.execute(\"SELECT COUNT(*) FROM failed_articles\")\n",
    "            stats['failed_articles'] = cursor.fetchone()[0]\n",
    "            \n",
    "            return stats\n",
    "\n",
    "    def _find_similar_articles(self, query_embedding: np.ndarray, limit: int) -> List[Dict]:\n",
    "        \"\"\"Find similar articles based on embedding.\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            cursor = conn.execute(\"\"\"\n",
    "                SELECT article_id, title, url, embedding\n",
    "                FROM embeddings\n",
    "            \"\"\")\n",
    "            \n",
    "            results = [\n",
    "                {\n",
    "                    'article_id': row[0],\n",
    "                    'title': row[1],\n",
    "                    'url': row[2],\n",
    "                    'similarity': float(np.dot(query_embedding[0], np.frombuffer(row[3], dtype=np.float32)))\n",
    "                }\n",
    "                for row in cursor\n",
    "            ]\n",
    "        \n",
    "        results.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "        return results[:limit]\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_embedding(self, input_data: Union[str, str]) -> torch.Tensor:\n",
    "        \"\"\"Generate embedding for the input data.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_similar_articles(self, input_data: Union[str, str], limit: int = 5) -> List[Dict]:\n",
    "        \"\"\"Find similar articles based on input data.\"\"\"\n",
    "        pass\n",
    "\n",
    "class TextMatcher(BaseEmbeddingsReader):\n",
    "    \"\"\"Reader for text-based embeddings.\"\"\"\n",
    "    \n",
    "    def get_embedding(self, input_text: str) -> torch.Tensor:\n",
    "        \"\"\"Generate embedding for input text.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            inputs = self.processor(\n",
    "                text=[input_text],\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=77\n",
    "            )\n",
    "            text_features = self.model.get_text_features(\n",
    "                **{k: v.to(self.device) for k, v in inputs.items()}\n",
    "            )\n",
    "            return text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    def get_similar_articles(self, query: str, limit: int = 5) -> List[Dict]:\n",
    "        \"\"\"Find articles similar to the query text.\"\"\"\n",
    "        query_embedding = self.get_embedding(query).cpu().numpy()\n",
    "        return self._find_similar_articles(query_embedding, limit)\n",
    "\n",
    "class ImageMatcher(BaseEmbeddingsReader):\n",
    "    \"\"\"Reader for image-based embeddings.\"\"\"\n",
    "\n",
    "    def _load_image(self, image_path: str) -> Image.Image:\n",
    "        \"\"\"Load an image from either a local path or URL.\"\"\"\n",
    "        try:\n",
    "            if image_path.startswith(('http://', 'https://')):\n",
    "                response = requests.get(image_path, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                return Image.open(BytesIO(response.content))\n",
    "            return Image.open(image_path)\n",
    "        except (requests.exceptions.RequestException, Exception) as e:\n",
    "            raise Exception(f\"Error loading image: {str(e)}\")\n",
    "\n",
    "    def get_embedding(self, image_path: str) -> torch.Tensor:\n",
    "        \"\"\"Generate embedding for input image.\"\"\"\n",
    "        try:\n",
    "            image = self._load_image(image_path)\n",
    "            with torch.no_grad():\n",
    "                inputs = self.processor(\n",
    "                    images=image,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                image_features = self.model.get_image_features(\n",
    "                    **{k: v.to(self.device) for k, v in inputs.items()}\n",
    "                )\n",
    "                return image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error processing image: {str(e)}\")\n",
    "\n",
    "    def get_similar_articles(self, image_path: str, limit: int = 5) -> List[Dict]:\n",
    "        \"\"\"Find articles similar to the input image.\"\"\"\n",
    "        query_embedding = self.get_embedding(image_path).cpu().numpy()\n",
    "        return self._find_similar_articles(query_embedding, limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print db stats\n",
    "text_reader = TextMatcher()\n",
    "image_reader = ImageMatcher()\n",
    "print(text_reader.get_database_stats())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For text-based search\n",
    "#similar_articles = text_reader.get_similar_articles(\"ficus audrey\")\n",
    "#[print(article['title']) for article in similar_articles];\n",
    "\n",
    "# For image-based search\n",
    "similar_articles = image_reader.get_similar_articles(\"/Users/clkruse/Downloads/IMG_6984.jpeg\",20)\n",
    "[print(f\"{article['title']}, {article['url']}, {article['similarity']:.3f}\") for article in similar_articles];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEmbeddingsReader(ABC):\n",
    "    \"\"\"Abstract base class for embedding readers.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 db_path: str = \"wiki_embeddings.db\",\n",
    "                 model_name: str = \"openai/clip-vit-base-patch32\"):\n",
    "        \"\"\"Initialize the base reader with common functionality.\"\"\"\n",
    "        self.db_path = db_path\n",
    "        self.device = self._get_device()\n",
    "        self.model = CLIPModel.from_pretrained(model_name).to(self.device)\n",
    "        self.processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_device() -> str:\n",
    "        \"\"\"Determine the appropriate device for computation.\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            return \"cuda\"\n",
    "        elif torch.backends.mps.is_available():\n",
    "            return \"mps\"\n",
    "        return \"cpu\"\n",
    "\n",
    "    def get_article_by_title(self, title: str) -> Optional[Dict]:\n",
    "        \"\"\"Get a specific article by its title.\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            cursor = conn.execute(\"\"\"\n",
    "                SELECT article_id, title, url, embedding, processed_date\n",
    "                FROM embeddings \n",
    "                WHERE title = ?\n",
    "            \"\"\", (title,))\n",
    "            \n",
    "            row = cursor.fetchone()\n",
    "            if row:\n",
    "                return {\n",
    "                    'article_id': row[0],\n",
    "                    'title': row[1],\n",
    "                    'url': row[2],\n",
    "                    'embedding': np.frombuffer(row[3], dtype=np.float32),\n",
    "                    'processed_date': row[4]\n",
    "                }\n",
    "            return None\n",
    "\n",
    "    def get_database_stats(self) -> Dict:\n",
    "        \"\"\"Get statistics about the database.\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            stats = {}\n",
    "            \n",
    "            cursor = conn.execute(\"SELECT COUNT(*) FROM embeddings\")\n",
    "            stats['total_articles'] = cursor.fetchone()[0]\n",
    "            \n",
    "            cursor = conn.execute(\"\"\"\n",
    "                SELECT title, processed_date \n",
    "                FROM embeddings \n",
    "                ORDER BY processed_date DESC \n",
    "                LIMIT 1\n",
    "            \"\"\")\n",
    "            row = cursor.fetchone()\n",
    "            if row:\n",
    "                stats['most_recent_article'] = {\n",
    "                    'title': row[0],\n",
    "                    'date': row[1]\n",
    "                }\n",
    "            \n",
    "            cursor = conn.execute(\"SELECT COUNT(*) FROM failed_articles\")\n",
    "            stats['failed_articles'] = cursor.fetchone()[0]\n",
    "            \n",
    "            return stats\n",
    "\n",
    "    def _find_similar_articles(self, query_embedding: np.ndarray, limit: int) -> List[Dict]:\n",
    "        \"\"\"Find similar articles based on embedding.\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            cursor = conn.execute(\"\"\"\n",
    "                SELECT article_id, title, url, embedding\n",
    "                FROM embeddings\n",
    "            \"\"\")\n",
    "            \n",
    "            results = [\n",
    "                {\n",
    "                    'article_id': row[0],\n",
    "                    'title': row[1],\n",
    "                    'url': row[2],\n",
    "                    'similarity': float(np.dot(query_embedding[0], np.frombuffer(row[3], dtype=np.float32)))\n",
    "                }\n",
    "                for row in cursor\n",
    "            ]\n",
    "        \n",
    "        results.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "        return results[:limit]\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_embedding(self, input_data: Union[str, str]) -> torch.Tensor:\n",
    "        \"\"\"Generate embedding for the input data.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_similar_articles(self, input_data: Union[str, str], limit: int = 5) -> List[Dict]:\n",
    "        \"\"\"Find similar articles based on input data.\"\"\"\n",
    "        pass\n",
    "\n",
    "class ImageMatcher(BaseEmbeddingsReader):\n",
    "    \"\"\"Reader for image-based embeddings.\"\"\"\n",
    "\n",
    "    def _load_image(self, image_path: str) -> Image.Image:\n",
    "        \"\"\"Load an image from either a local path or URL.\"\"\"\n",
    "        try:\n",
    "            if image_path.startswith(('http://', 'https://')):\n",
    "                response = requests.get(image_path, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                return Image.open(BytesIO(response.content))\n",
    "            return Image.open(image_path)\n",
    "        except (requests.exceptions.RequestException, Exception) as e:\n",
    "            raise Exception(f\"Error loading image: {str(e)}\")\n",
    "\n",
    "    def get_embedding(self, image_path: str) -> torch.Tensor:\n",
    "        \"\"\"Generate embedding for input image.\"\"\"\n",
    "        try:\n",
    "            image = self._load_image(image_path)\n",
    "            with torch.no_grad():\n",
    "                inputs = self.processor(\n",
    "                    images=image,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                image_features = self.model.get_image_features(\n",
    "                    **{k: v.to(self.device) for k, v in inputs.items()}\n",
    "                )\n",
    "                return image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error processing image: {str(e)}\")\n",
    "\n",
    "    def get_similar_articles(self, image_path: str, limit: int = 5) -> List[Dict]:\n",
    "        \"\"\"Find articles similar to the input image.\"\"\"\n",
    "        query_embedding = self.get_embedding(image_path).cpu().numpy()\n",
    "        return self._find_similar_articles(query_embedding, limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "core",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
